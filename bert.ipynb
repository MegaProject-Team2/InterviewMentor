{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load train and test datasets\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Inspect the data\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect rows with missing answers\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def find_closest_answer(context, answer, threshold=0.6):\n",
    "    words = context.split()  # Tokenize context into words\n",
    "    closest_match = None\n",
    "    max_similarity = 0\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words) + 1):\n",
    "            substring = \" \".join(words[i:j])\n",
    "            similarity = difflib.SequenceMatcher(None, substring, answer).ratio()\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                closest_match = (substring, i, j - 1)  # Save token indices\n",
    "    \n",
    "    return closest_match if max_similarity >= threshold else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "def preprocess_data(data):\n",
    "    inputs = []\n",
    "    for _, row in data.iterrows():\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            context,\n",
    "            question,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Use approximate matching\n",
    "        match = find_closest_answer(context, answer)\n",
    "        if not match:\n",
    "            print(f\"Answer '{answer}' not found even with approximate matching!\")\n",
    "            continue\n",
    "\n",
    "        substring, start_word_idx, end_word_idx = match\n",
    "        start_char_idx = context.find(substring)\n",
    "        end_char_idx = start_char_idx + len(substring) - 1\n",
    "\n",
    "        start_token_idx = encoded.char_to_token(0, start_char_idx)\n",
    "        end_token_idx = encoded.char_to_token(0, end_char_idx)\n",
    "\n",
    "        if start_token_idx is None or end_token_idx is None:\n",
    "            continue\n",
    "\n",
    "        inputs.append({\n",
    "            'input_ids': encoded['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
    "            'start_positions': start_token_idx,\n",
    "            'end_positions': end_token_idx\n",
    "        })\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# Function to preprocess data\n",
    "# def preprocess_data(data):\n",
    "#     inputs = []\n",
    "#     for _, row in data.iterrows():\n",
    "#         context = row['context']\n",
    "#         question = row['question']\n",
    "#         answer = row['answer']\n",
    "\n",
    "#         print(f\"Processing: \\nContext: {context[:100]}...\\nQuestion: {question}\\nAnswer: {answer}\")\n",
    "\n",
    "#         # Tokenize context and question\n",
    "#         encoded = tokenizer(\n",
    "#             context,\n",
    "#             question,\n",
    "#             max_length=512,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         # Find character indices\n",
    "#         start_char_idx = context.find(answer)\n",
    "#         if start_char_idx == -1:\n",
    "#             print(f\"Answer '{answer}' not found in context!\")\n",
    "#             continue\n",
    "\n",
    "#         end_char_idx = start_char_idx + len(answer) - 1\n",
    "\n",
    "#         # Map character indices to token indices\n",
    "#         start_token_idx = encoded.char_to_token(0, start_char_idx)\n",
    "#         end_token_idx = encoded.char_to_token(0, end_char_idx)\n",
    "\n",
    "#         if start_token_idx is None or end_token_idx is None:\n",
    "#             print(f\"Token indices not found for answer: {answer}\")\n",
    "#             continue\n",
    "\n",
    "#         inputs.append({\n",
    "#             'input_ids': encoded['input_ids'].squeeze(),\n",
    "#             'attention_mask': encoded['attention_mask'].squeeze(),\n",
    "#             'start_positions': start_token_idx,\n",
    "#             'end_positions': end_token_idx\n",
    "#         })\n",
    "\n",
    "#     print(f\"Number of valid samples: {len(inputs)}\")\n",
    "\n",
    "#     return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, row in train_data.iterrows():\n",
    "#     context = row['context']\n",
    "#     answer = row['answer']\n",
    "#     if context.find(answer) == -1:\n",
    "#         print(f\"Context: {context[:100]}...\\nAnswer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "train_encodings = preprocess_data(train_data)\n",
    "test_encodings = preprocess_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.encodings[idx]['input_ids'],\n",
    "            'attention_mask': self.encodings[idx]['attention_mask'],\n",
    "            'start_positions': torch.tensor(self.encodings[idx]['start_positions']),\n",
    "            'end_positions': torch.tensor(self.encodings[idx]['end_positions'])\n",
    "        }\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QADataset(train_encodings)\n",
    "test_dataset = QADataset(test_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "# Load pretrained BERT model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(context, question):\n",
    "    inputs = tokenizer(\n",
    "        context,\n",
    "        question,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Get model predictions\n",
    "    outputs = model(**inputs)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Find the start and end positions\n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits) + 1\n",
    "\n",
    "    # Decode the predicted tokens\n",
    "    predicted_answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx])\n",
    "    )\n",
    "    return predicted_answer\n",
    "\n",
    "# Example\n",
    "context = \"Hugging Face is creating tools for NLP and machine learning.\"\n",
    "question = \"What is Hugging Face creating?\"\n",
    "print(predict(context, question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
